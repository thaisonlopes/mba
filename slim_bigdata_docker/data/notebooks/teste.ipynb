{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.cartolafc.globo.com/partidas/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "with open('test_cartola_outputfile.json', 'wb') as outf:\n",
    "    outf.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teste' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8ee9bef761d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_cartola_outputfile.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'teste' is not defined"
     ]
    }
   ],
   "source": [
    "data = json.load(open('test_cartola_outputfile.json'))\n",
    "\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_corrupt_record,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = sqlContext.read.json(json.dumps(data))\n",
    "\n",
    "df = spark.read.json(sc.parallelize([data]))\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2893b78fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colocando os arquivos da pasta cartola no HDFS:\n",
    "### hadoop fs -put /cartola/* /cartola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lances_2014_df = spark.read.csv(\"/cartola/data/2014/2014_lances.csv\", header=True)\n",
    "sorted_lances_2014_df = lances_2014_df.sort(lances_2014_df.ID.desc())\n",
    "\n",
    "times_2014_df = spark.read.csv(\"/cartola/data/2014/2014_times.csv\", header=True)\n",
    "sorted_times_2014_df = times_2014_df.sort(times_2014_df.ID.desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,StringType,true),StructField(Nome,StringType,true),StructField(Abreviacao,StringType,true),StructField(Slug,StringType,true)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_lances_2014_df.schema\n",
    "sorted_times_2014_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+--------+-------+-------+----+\n",
      "|       ID|PartidaID|ClubeID|AtletaID|Periodo|Momento|Tipo|\n",
      "+---------+---------+-------+--------+-------+-------+----+\n",
      "|141622764|   180149|    275|   38435|    2tr|     38|  ZB|\n",
      "|141622763|   180149|    275|   87420|    2tr|     24|  ZF|\n",
      "|141622762|   180149|    275|   41519|    2tr|     24|  ZB|\n",
      "|141622761|   180149|    275|   88368|    1tr|     18|  ZB|\n",
      "|141622760|   180149|    293|   87009|    1tr|     10|  ZB|\n",
      "|141622759|   180152|    266|   50417|    2tr|     45|  CA|\n",
      "|141622758|   180152|    266|   37627|    2tr|     38|  CA|\n",
      "|141622757|   180152|    266|   74059|    2tr|     17|  CA|\n",
      "|141622756|   180152|    266|   80370|    1tr|     40|  CA|\n",
      "|141622755|   180152|    283|   42116|    1tr|     16|  CA|\n",
      "|141622754|   180152|    283|   50358|    2tr|     14|  ZG|\n",
      "|141622753|   180152|    283|   38573|    1tr|     44|  ZG|\n",
      "|141622752|   180152|    266|   38162|    1tr|     33|  ZG|\n",
      "|141622751|   180158|    284|   78715|    2tr|     28|  CA|\n",
      "|141622750|   180158|    284|   42501|    2tr|     21|  CA|\n",
      "|141622749|   180158|    262|   78086|    2tr|     11|  CV|\n",
      "|141622748|   180158|    284|   62086|    2tr|      9|  CA|\n",
      "|141622747|   180158|    284|   86759|    2tr|     13|  ZG|\n",
      "|141622746|   180158|    262|   74060|    1tr|     31|  ZG|\n",
      "|141622739|   180154|    277|   68929|    2tr|     44|  IM|\n",
      "+---------+---------+-------+--------+-------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "73113\n"
     ]
    }
   ],
   "source": [
    "sorted_lances_2014_df.show()\n",
    "\n",
    "print(sorted_lances_2014_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+-------------+\n",
      "| ID|         Nome|Abreviacao|         Slug|\n",
      "+---+-------------+----------+-------------+\n",
      "|316|  figueirense|       FIG|  figueirense|\n",
      "|315|  chapecoense|       CHA|  chapecoense|\n",
      "|294|     coritiba|       CFC|     coritiba|\n",
      "|293|  atlético-pr|       CAP|  atletico-pr|\n",
      "|292|        sport|       SPO|        sport|\n",
      "|290|        goiás|       GOI|        goias|\n",
      "|288|     criciúma|       CRI|     criciuma|\n",
      "|287|      vitória|       VIT|      vitoria|\n",
      "|285|internacional|       INT|internacional|\n",
      "|284|       grêmio|       GRE|       gremio|\n",
      "|283|     cruzeiro|       CRU|     cruzeiro|\n",
      "|282|  atlético-mg|       CAM|  atletico-mg|\n",
      "|277|       santos|       SAN|       santos|\n",
      "|276|    são paulo|       SAO|    sao-paulo|\n",
      "|275|    palmeiras|       PAL|    palmeiras|\n",
      "|266|   fluminense|       FLU|   fluminense|\n",
      "|265|        bahia|       BAH|        bahia|\n",
      "|264|  corinthians|       COR|  corinthians|\n",
      "|263|     botafogo|       BOT|     botafogo|\n",
      "|262|     flamengo|       FLA|     flamengo|\n",
      "+---+-------------+----------+-------------+\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "sorted_times_2014_df.show()\n",
    "\n",
    "print(sorted_times_2014_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo todas as partidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds_partidas_df = spark.read.csv(\"/cartola/data/*/*_partidas.csv\", header=True)\n",
    "\n",
    "tds_partidas_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_sorted_partidas_df = tds_partidas_df.sort(tds_partidas_df._c0.desc())\n",
    "\n",
    "desc_sorted_partidas_df.show(10)\n",
    "\n",
    "desc_sorted_partidas_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_sorted_partidas_df = tds_partidas_df.sort(tds_partidas_df._c0.asc())\n",
    "\n",
    "asc_sorted_partidas_df.show(10)\n",
    "\n",
    "asc_sorted_partidas_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame Row Object to a new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert spark DataFrame column of type list to new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atletas_test = df.select(\"atletas\").rdd.flatMap(lambda x: x[0]).collect()\n",
    "\n",
    "\n",
    "atletas_df = spark.createDataFrame(atletas_test) \n",
    "atletas_df.head(3)\n",
    "\n",
    "# Onde o novo dataframe foi salvo?\n",
    "# atletas_df.write.json(\"teste/atletas_teste.json\")\n",
    "atletas_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atletas_df.createOrReplaceTempView(\"tempAtletas\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /user/hive/warehouse/teste_tabela_atletas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"create table atletas_hive_test as select * from tempAtletas\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"show tables from default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE atletas (apelido STRING, atleta_id BIGINT)\n",
    "  STORED AS PARQUET LOCATION '/user/hive/warehouse/teste_tabela_atletas/';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlContext.sql(\"show databases\").show()\n",
    "\n",
    "sqlContext.sql(\"CREATE EXTERNAL TABLE atletas (apelido STRING, atleta_id BIGINT) \\\n",
    "               STORED AS PARQUET LOCATION '/user/root/teste/atletas_teste.parquet' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setProperty(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.set(\"spark.executor.uri\", \"thrift://hive-metastore:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.facebook.presto.jdbc.PrestoDriver\") \\\n",
    "    .option(\"url\", \"jdbc:presto://presto:8080/hive/default\") \\\n",
    "    .option(\"user\", \"hive\") \\\n",
    "    .option(\"dbtable\", \"pessoas\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
